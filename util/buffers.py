import collections
import numpy as np

# returns a namedtuple with (state, action, reward...)
Experience = collections.namedtuple("Experience", field_names=['state', 'action', 'reward', 'done', 'new_state'])


class ExperienceBuffer:
    """
    Buffer of fixed capacity to handle previous experiences for bootstrap sampling.
    """
    def __init__(self, capacity: int):
        self.buffer = collections.deque(maxlen=capacity)
        self.capacity = capacity

    def __len__(self):
        return len(self.buffer)

    def append(self, experience: Experience):
        self.buffer.append(experience)

    # [self.buffer[idx] for idx in indices] is a list of [exp, exp,..., exp]
    # *[exp,exp,exp] returns then exp, exp, exp as arguments to a function
    # zip(exp, exp, exp) then rearranges the tuples such that new tuples are created
    # where the first elements of every exp tuple is in one tuple,
    # the second elements of every exp tuple are in one tuple etc
    # TODO standardize variable type declarations
    def sample(self, batch_size: int):
        """
        Sample random batch of experiences from buffer.
        :param batch_size: int
        :return: states, actions, rewards, dones, next_states
        """
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])
        return states, np.array(actions), np.array(rewards, dtype=np.float32), \
               np.array(dones, dtype=np.bool), np.array(next_states)


    def fill(self, env, filllength, nstep):
        """
        fill the buffer with experiences from random actions generated by environment.
        :param env: gym environment
        :return: None
        """
        # nsamples = self.buffer.maxlen
        nsamples = filllength * nstep
        state = env.reset()
        state = np.array(state, copy=True)
        for frames in range(nsamples):
            action = env.action_space.sample()
            new_state, reward, is_done, _ = env.step(action)
            new_state = np.array(new_state, copy=True)
            exp = Experience(state, action, reward, is_done, new_state)
            self.append(exp)
            state = new_state
            if is_done:
                state = env.reset()
                state = np.array(state, copy=True)


class Extendedbuffer (ExperienceBuffer):
    def __init__(self, capacity, nstep=1, gamma=0.99):
        super(Extendedbuffer, self).__init__(nstep)
        self.exp_buffer = ExperienceBuffer(capacity)
        self.gamma = gamma

    def append(self, experience):
        self.buffer.append(experience)
        if not len(self) < self.capacity:
            state = self.buffer[0][0]
            action = self.buffer[0][1]
            reward, done, next_state = self.buffer[-1][-3:]
            for exp in reversed(list(self.buffer)[:-1]):
                r, d, ns = exp[-3:]
                reward = r + self.gamma * reward * (1-d)
                next_state, done = (ns, d) if d else (next_state, done)
            self.exp_buffer.append(Experience(state, action, reward, done, next_state))
            self.buffer.clear()

    def sample(self, batchsize: int):
        return self.exp_buffer.sample(batchsize)

